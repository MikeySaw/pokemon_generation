{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Gradient with respect to W_ph:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`∂L(T)/∂W_ph = ∂L(T)/∂p(T) * ∂p(T)/∂W_ph\n",
    "= ∂L(T)/∂p(T) * h(T)^T`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Gradient with respect to W_hh:`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`∂L(T)/∂W_hh = Σ(t=1 to T) ∂L(T)/∂h(T) * ∂h(T)/∂h(t) * ∂h(t)/∂W_hh`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think generally speaking the second one would have a exploding gradient and a vanishing gradient problem, also for long term dependencies it may not work anymore since the vanishing gradient will not carry on the information anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, seq_length, input_dim, num_hidden, num_classes, batch_size, device=\"cpu\"):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.num_hidden = num_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_hx = nn.Parameter(torch.randn(num_hidden, input_dim, device=device))\n",
    "        self.W_hh = nn.Parameter(torch.randn(num_hidden, num_hidden, device=device))\n",
    "        self.b_h = nn.Parameter(torch.zeros(num_hidden, device=device))\n",
    "        \n",
    "        self.W_ph = nn.Parameter(torch.randn(num_classes, num_hidden, device=device))\n",
    "        self.b_p = nn.Parameter(torch.zeros(num_classes, device=device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, input_dim)\n",
    "        h = torch.zeros(self.batch_size, self.num_hidden, device=self.device)\n",
    "        \n",
    "        outputs = []\n",
    "\n",
    "        for t in range(self.seq_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h = torch.tanh(torch.matmul(x_t, self.W_hx.t()) + torch.matmul(h, self.W_hh.t()) + self.b_h)\n",
    "            p = torch.matmul(h, self.W_ph.t()) + self.b_p\n",
    "            outputs.append(p)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if config.model_type == \"RNN\":\n",
    "    model = VanillaRNN(\n",
    "        seq_length=config.input_length,\n",
    "        input_dim=config.input_dim,\n",
    "        num_hidden=config.num_hidden,\n",
    "        num_classes=config.num_classes,\n",
    "        batch_size=config.batch_size,\n",
    "        device=device\n",
    "    ).to(device)\n",
    "elif config.model_type == \"LSTM\":\n",
    "    model = LSTM(\n",
    "        seq_length=config.input_length,\n",
    "        input_dim=config.input_dim,\n",
    "        num_hidden=config.num_hidden,\n",
    "        num_classes=config.num_classes,\n",
    "        batch_size=config.batch_size,\n",
    "        device=device\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2018\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to conditions.\n",
    "#\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from part1.dataset import PalindromeDataset\n",
    "from part1.vanilla_rnn import VanillaRNN\n",
    "from part1.lstm import LSTM\n",
    "\n",
    "# You may want to look into tensorboardX for logging\n",
    "# from tensorboardX import SummaryWriter\n",
    "\n",
    "################################################################################\n",
    "\n",
    "\n",
    "def train(config):\n",
    "\n",
    "    assert config.model_type in (\"RNN\", \"LSTM\")\n",
    "\n",
    "    # Initialize the device which to run the model on\n",
    "    device = torch.device(config.device)\n",
    "\n",
    "    # Initialize the model that we are going to use\n",
    "    if config.model_type == \"RNN\":\n",
    "    model = VanillaRNN(\n",
    "        seq_length=config.input_length,\n",
    "        input_dim=config.input_dim,\n",
    "        num_hidden=config.num_hidden,\n",
    "        num_classes=config.num_classes,\n",
    "        batch_size=config.batch_size,\n",
    "        device=device\n",
    "    ).to(device)\n",
    "    elif config.model_type == \"LSTM\":\n",
    "        model = LSTM(\n",
    "            seq_length=config.input_length,\n",
    "            input_dim=config.input_dim,\n",
    "            num_hidden=config.num_hidden,\n",
    "            num_classes=config.num_classes,\n",
    "            batch_size=config.batch_size,\n",
    "            device=device\n",
    "        ).to(device)  # fixme\n",
    "\n",
    "    # Initialize the dataset and data loader (note the +1)\n",
    "    dataset = PalindromeDataset(config.input_length + 1)\n",
    "    data_loader = DataLoader(dataset, config.batch_size, num_workers=1)\n",
    "\n",
    "    # Setup the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()  # fixme\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate)  # fixme\n",
    "\n",
    "    for step, (batch_inputs, batch_targets) in enumerate(data_loader):\n",
    "\n",
    "        # Only for time measurement of step through network\n",
    "        t1 = time.time()\n",
    "\n",
    "        # Add more code here ...\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        batch_targets = batch_targets.to(device)\n",
    "        \n",
    "        outputs = model(batch_inputs)\n",
    "        ############################################################################\n",
    "        # QUESTION: what happens here and why?\n",
    "        ############################################################################\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=config.max_norm)\n",
    "        ############################################################################\n",
    "\n",
    "        # Add more code here ...\n",
    "        loss = criterion(outputs[:, -1, :], batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss = np.inf  # fixme\n",
    "        accuracy = 0.0  # fixme\n",
    "\n",
    "        # Just for time measurement\n",
    "        t2 = time.time()\n",
    "        examples_per_second = config.batch_size / float(t2 - t1)\n",
    "\n",
    "        if step % 10 == 0:\n",
    "\n",
    "            print(\n",
    "                \"[{}] Train Step {:04d}/{:04d}, Batch Size = {}, Examples/Sec = {:.2f}, \"\n",
    "                \"Accuracy = {:.2f}, Loss = {:.3f}\".format(\n",
    "                    datetime.now().strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                    step,\n",
    "                    config.train_steps,\n",
    "                    config.batch_size,\n",
    "                    examples_per_second,\n",
    "                    accuracy,\n",
    "                    loss,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if step == config.train_steps:\n",
    "            # If you receive a PyTorch data-loader error, check this bug report:\n",
    "            # https://github.com/pytorch/pytorch/pull/9655\n",
    "            break\n",
    "\n",
    "    print(\"Done training.\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "################################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Parse training configuration\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Model params\n",
    "    parser.add_argument(\n",
    "        \"--model_type\",\n",
    "        type=str,\n",
    "        default=\"RNN\",\n",
    "        help=\"Model type, should be 'RNN' or 'LSTM'\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_length\", type=int, default=10, help=\"Length of an input sequence\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--input_dim\", type=int, default=1, help=\"Dimensionality of input sequence\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_classes\", type=int, default=10, help=\"Dimensionality of output sequence\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_hidden\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Number of hidden units in the model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help=\"Number of examples to process in a batch\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", type=float, default=0.001, help=\"Learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_steps\", type=int, default=10000, help=\"Number of training steps\"\n",
    "    )\n",
    "    parser.add_argument(\"--max_norm\", type=float, default=10.0)\n",
    "    parser.add_argument(\n",
    "        \"--device\", type=str, default=\"cuda:0\", help=\"Training device 'cpu' or 'cuda:0'\"\n",
    "    )\n",
    "\n",
    "    config = parser.parse_args()\n",
    "\n",
    "    # Train the model\n",
    "    train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from part1.dataset import PalindromeDataset\n",
    "from part1.vanilla_rnn import VanillaRNN\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_and_evaluate(seq_length, config):\n",
    "    # Initialize the model\n",
    "    model = VanillaRNN(\n",
    "        seq_length=seq_length,\n",
    "        input_dim=config.input_dim,\n",
    "        num_hidden=config.num_hidden,\n",
    "        num_classes=config.num_classes,\n",
    "        batch_size=config.batch_size,\n",
    "        device=config.device\n",
    "    ).to(config.device)\n",
    "\n",
    "    # Initialize the dataset and data loader\n",
    "    dataset = PalindromeDataset(seq_length + 1)\n",
    "    data_loader = DataLoader(dataset, config.batch_size, num_workers=1)\n",
    "\n",
    "    # Setup the loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for step, (batch_inputs, batch_targets) in enumerate(data_loader):\n",
    "        batch_inputs = batch_inputs.to(config.device)\n",
    "        batch_targets = batch_targets.to(config.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs[:, -1, :], batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step >= config.train_steps:\n",
    "            break\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in data_loader:\n",
    "            batch_inputs = batch_inputs.to(config.device)\n",
    "            batch_targets = batch_targets.to(config.device)\n",
    "            outputs = model(batch_inputs)\n",
    "            _, predicted = torch.max(outputs[:, -1, :], 1)\n",
    "            total += batch_targets.size(0)\n",
    "            correct += (predicted == batch_targets).sum().item()\n",
    "            if total >= 1000:  # Evaluate on 1000 samples\n",
    "                break\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Experiment with increasing sequence lengths\n",
    "seq_lengths = range(5, 21, 2)  # [5, 7, 9, ..., 19]\n",
    "accuracies = []\n",
    "\n",
    "for seq_length in seq_lengths:\n",
    "    accuracy = train_and_evaluate(seq_length, config)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f\"Sequence length: {seq_length}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_lengths, accuracies, marker='o')\n",
    "plt.title('Accuracy vs Palindrome Length for Vanilla RNN')\n",
    "plt.xlabel('Palindrome Length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.savefig('vanilla_rnn_accuracy.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, seq_length, input_dim, num_hidden, num_classes, batch_size, device=\"cpu\"):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.seq_length = seq_length\n",
    "        self.num_hidden = num_hidden\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W_gx = nn.Parameter(torch.randn(num_hidden, input_dim, device=device))\n",
    "        self.W_gh = nn.Parameter(torch.randn(num_hidden, num_hidden, device=device))\n",
    "        self.b_g = nn.Parameter(torch.zeros(num_hidden, device=device))\n",
    "\n",
    "        self.W_ix = nn.Parameter(torch.randn(num_hidden, input_dim, device=device))\n",
    "        self.W_ih = nn.Parameter(torch.randn(num_hidden, num_hidden, device=device))\n",
    "        self.b_i = nn.Parameter(torch.zeros(num_hidden, device=device))\n",
    "\n",
    "        self.W_fx = nn.Parameter(torch.randn(num_hidden, input_dim, device=device))\n",
    "        self.W_fh = nn.Parameter(torch.randn(num_hidden, num_hidden, device=device))\n",
    "        self.b_f = nn.Parameter(torch.zeros(num_hidden, device=device))\n",
    "\n",
    "        self.W_ox = nn.Parameter(torch.randn(num_hidden, input_dim, device=device))\n",
    "        self.W_oh = nn.Parameter(torch.randn(num_hidden, num_hidden, device=device))\n",
    "        self.b_o = nn.Parameter(torch.zeros(num_hidden, device=device))\n",
    "\n",
    "        self.W_ph = nn.Parameter(torch.randn(num_classes, num_hidden, device=device))\n",
    "        self.b_p = nn.Parameter(torch.zeros(num_classes, device=device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, input_dim)\n",
    "        h = torch.zeros(self.batch_size, self.num_hidden, device=self.device)\n",
    "        c = torch.zeros(self.batch_size, self.num_hidden, device=self.device)\n",
    "        \n",
    "        outputs = []\n",
    "\n",
    "        for t in range(self.seq_length):\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            g = torch.tanh(torch.matmul(x_t, self.W_gx.t()) + torch.matmul(h, self.W_gh.t()) + self.b_g)\n",
    "            i = torch.sigmoid(torch.matmul(x_t, self.W_ix.t()) + torch.matmul(h, self.W_ih.t()) + self.b_i)\n",
    "            f = torch.sigmoid(torch.matmul(x_t, self.W_fx.t()) + torch.matmul(h, self.W_fh.t()) + self.b_f)\n",
    "            o = torch.sigmoid(torch.matmul(x_t, self.W_ox.t()) + torch.matmul(h, self.W_oh.t()) + self.b_o)\n",
    "            \n",
    "            c = g * i + c * f\n",
    "            h = torch.tanh(c) * o\n",
    "            \n",
    "            p = torch.matmul(h, self.W_ph.t()) + self.b_p\n",
    "            outputs.append(p)\n",
    "\n",
    "        return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from part1.dataset import PalindromeDataset\n",
    "from part1.vanilla_rnn import VanillaRNN\n",
    "from part1.lstm import LSTM\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_and_evaluate(model_type, seq_length, config):\n",
    "    if model_type == \"RNN\":\n",
    "        model = VanillaRNN(seq_length, config.input_dim, config.num_hidden, config.num_classes, config.batch_size, config.device).to(config.device)\n",
    "    elif model_type == \"LSTM\":\n",
    "        model = LSTM(seq_length, config.input_dim, config.num_hidden, config.num_classes, config.batch_size, config.device).to(config.device)\n",
    "\n",
    "    dataset = PalindromeDataset(seq_length + 1)\n",
    "    data_loader = DataLoader(dataset, config.batch_size, num_workers=1)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "    # Training loop (same as before)\n",
    "    # ...\n",
    "\n",
    "    # Evaluation (same as before)\n",
    "    # ...\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Experiment with increasing sequence lengths\n",
    "seq_lengths = range(5, 31, 5)  # [5, 10, 15, 20, 25, 30]\n",
    "rnn_accuracies = []\n",
    "lstm_accuracies = []\n",
    "\n",
    "for seq_length in seq_lengths:\n",
    "    rnn_accuracy = train_and_evaluate(\"RNN\", seq_length, config)\n",
    "    lstm_accuracy = train_and_evaluate(\"LSTM\", seq_length, config)\n",
    "    rnn_accuracies.append(rnn_accuracy)\n",
    "    lstm_accuracies.append(lstm_accuracy)\n",
    "    print(f\"Sequence length: {seq_length}\")\n",
    "    print(f\"RNN Accuracy: {rnn_accuracy:.4f}\")\n",
    "    print(f\"LSTM Accuracy: {lstm_accuracy:.4f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(seq_lengths, rnn_accuracies, marker='o', label='RNN')\n",
    "plt.plot(seq_lengths, lstm_accuracies, marker='s', label='LSTM')\n",
    "plt.title('Accuracy vs Palindrome Length: RNN vs LSTM')\n",
    "plt.xlabel('Palindrome Length')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('rnn_vs_lstm_accuracy.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
